{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSI 4106 Introduction to Artificial Intelligence \n",
    "## Assignment 3: Neural Networks\n",
    "\n",
    "## Report Title: Implementing Neural Networks, Tuning Hyperparameters and Evaluating Models for Machine Learning\n",
    "\n",
    "### Identification\n",
    "\n",
    "Name: Alex Govier <br/>\n",
    "Student Number: 300174954"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis\n",
    "\n",
    "#### 1. Loading Dataset and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the three datasets from my GitHub\n",
    "test = \"https://github.com/alex-govier5/intro-to-ai/raw/master/A3/cb513_test.csv\"\n",
    "test_set = pd.read_csv(test)\n",
    "\n",
    "train = \"https://github.com/alex-govier5/intro-to-ai/raw/master/A3/cb513_train.csv\"\n",
    "training_set = pd.read_csv(train)\n",
    "\n",
    "valid = \"https://github.com/alex-govier5/intro-to-ai/raw/master/A3/cb513_valid.csv\"\n",
    "valid_set = pd.read_csv(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I load my data sets and can see that the training set includes 58,291 examples, the validation set contains 7,409 examples, and the test set has 7,432 examples. The target variable in the first column, can take on one of three values: 0, 1, or 2. The remaining 462 columns represent attributes, which are numerical values ranging from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Shuffling the Rows\n",
    "Here I will shuffle my rows to mitigate the potential negative impact on model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the rows of the datasets\n",
    "test_set = test_set.sample(frac=1).reset_index(drop=True)\n",
    "training_set = training_set.sample(frac=1).reset_index(drop=True)\n",
    "valid_set = valid_set.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frac=1 part means that 100% of the data is being shuffled. The reset_index makes sure that the index is reset after shuffling. So this should help with the adjacent examples problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Isolating the Target and the Data\n",
    "Here I will isolate the target and separate it from the features before I scale any features so that the target variable does not get scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y) for training set\n",
    "y_train = training_set.iloc[:, 0]     # Only the first column\n",
    "X_train = training_set.iloc[:, 1:]    # All columns except the first\n",
    "\n",
    "# Separate features (X) and target (y) for validation set\n",
    "y_valid = valid_set.iloc[:, 0]        # Only the first column\n",
    "X_valid = valid_set.iloc[:, 1:]       # All columns except the first\n",
    "\n",
    "# Separate features (X) and target (y) for test set\n",
    "y_test = test_set.iloc[:, 0]          # Only the first column\n",
    "X_test = test_set.iloc[:, 1:]         # All columns except the first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the three datasets are now separated into the features (x) and the target (y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Scaling the Numerical Features\n",
    "Here I will scale my features for one dataset so that it will act as one experiment I can use later on to compare performance with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the train features to use as experiment later on\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here I use the min max scaler to fit transform one of my dataset features, I chose the X_train set. Since X_train contains all my coumns, the scaling will be applied uniformly across all columns. I will use this scaled set later on to see if it improves any performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Model Development\n",
    "Here I will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy model that predicts the majority class\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "# Baseline model, logistic regression\n",
    "baseline_clf = LogisticRegression(max_iter=200)\n",
    "baseline_clf.fit(X_train, y_train)\n",
    "\n",
    "# Neural network \n",
    "nn_model = Sequential([\n",
    "    Input(shape=(462,)),             # Input layer\n",
    "    Dense(8),                        # Hidden layer with 8 nodes\n",
    "    Dense(3, activation='softmax')   # Output layer with 3 nodes for 3 classes\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I implement my dummy model which will predict the majority class. Then I chose to implement logistic regression because it tends to work well with high dimensionality which my datasets have, it uses probabilistic interpretation, so it provides a natural way to interpret the likelihood of each structure. It can handle scaled data well which will be useful for my scaled dataset experiment, and it is a relatively simple model that is fast to train. I then implement the neural network with tensorflow and keras, with an input layer with 462 nodes, then the hidden layer with 8 nodes and the default activation function (so not specified) and finally the output layer with 3 nodes using the softmax activation function. So my three models are setup for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Model Evaluation\n",
    "Here I will evaluate my models with cross validation for the baseline, and using the validation set for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Baseline Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Model Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[Matplotlib Pyplot Documentation](https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html)<br/>\n",
    "[Numpy User Guide](https://numpy.org/devdocs/user/)<br/>\n",
    "[Pandas User Guide](https://pandas.pydata.org/docs/user_guide/index.html) <br/>\n",
    "[Seaborn User Guide](https://seaborn.pydata.org/tutorial/introduction.html)<br/>\n",
    "[Sklearn Linear Model Documentation](https://scikit-learn.org/stable/modules/linear_model.html)<br/>\n",
    "[Sklearn Metrics Documentation](https://scikit-learn.org/stable/api/sklearn.metrics.html)<br/>\n",
    "[Sklearn Model Selection Documentation](https://scikit-learn.org/stable/api/sklearn.model_selection.html)<br/>\n",
    "[Sklearn Neighbors Documentation](https://scikit-learn.org/stable/modules/neighbors.html)<br/>\n",
    "[Sklearn Preprocessing Documentation](https://scikit-learn.org/stable/modules/preprocessing.html)<br/>\n",
    "[Sklearn Tree Documentation](https://scikit-learn.org/stable/modules/tree.html)<br/>\n",
    "<br/>\n",
    "Most of my reference came from my own first assignment since a lot of the techniques were able to be used for this assignment as well. For the newer concepts I referred to the documentation and the course lecture notes to see how to implement them. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
